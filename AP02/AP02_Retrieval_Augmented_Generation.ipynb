{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1845a46",
   "metadata": {},
   "source": [
    "# Arbeitspaket (AP) 2: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "**Datenthema:** Jugend, Politik, Umwelt & Partizipation\n",
    "  \n",
    "**Maximale Punktzahl:** 60 Punkte\n",
    "\n",
    "---\n",
    "## Hinweise\n",
    "- Beantworten Sie alle Aufgaben direkt in diesem Notebook.\n",
    "- Ändern Sie Zellen mit dem Hinweis **\"Bitte nicht verändern\"** nicht.\n",
    "- Verwenden Sie, wo möglich, die bereits gegebenen Variablen und Funktionen.\n",
    "- Antworten auf Theoriefragen schreiben Sie in Markdown-Zellen.\n",
    "\n",
    "Die Daten für den RAG-Teil (Texte zu *Jugend, Politik, Umwelt & Partizipation* in deutscher Sprache) müsssen im Ordner `data/` abgelegt werden. Sie benötigen ebenfalls eine `.env`-Datei für die Keys.\n",
    "\n",
    "---\n",
    "## Teil A – LLM API-Aufrufe (25 Punkte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f44ee",
   "metadata": {},
   "source": [
    "### Persönliche Angaben (bitte ergänzen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb046ae0",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td>Vorname:</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nachname:</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Immatrikulationsnummer:</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Modul:</td>\n",
    "    <td>Data Science</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Prüfungsdatum / Raum / Zeit:</td>\n",
    "    <td>15.12.2025 / Raum: MU O2.001 / 8:00 – 11:45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Erlaubte Hilfsmittel:</td>\n",
    "    <td>w.MA.XX.DS.24HS (Data Science)<br>Open Book, Eigener Computer, Internet-Zugang</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Nicht erlaubt:</td>\n",
    "  <td>Nicht erlaubt ist der Einsatz beliebiger Formen von generativer KI (z.B. Copilot, ChatGPT) <br> sowie beliebige Formen von Kommunikation oder Kollaboration mit anderen Menschen.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395fab0",
   "metadata": {},
   "source": [
    "## Bewertungskriterien\n",
    "\n",
    "| **Kategorie**                       | **Beschreibung**                                                                                                                                          | **Punkteverteilung**                |\n",
    "|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|\n",
    "| **Code nicht lauffähig oder Ergebnisse nicht relevant** | Der Code läuft nicht oder erfüllt nicht die Anforderungen des Aspekts (z. B. Bilder werden nicht geladen, die Textausgabe der Extraktion fehlt, Bounding Boxes werden nicht angezeigt). | **0 Punkte**                       |\n",
    "| **Code lauffähig, aber mit gravierenden Mängeln**       | Der Code läuft, jedoch fehlen zentrale Teile der Funktionalität eines Aspekts (z. B. unvollständige Extraktion von Bildinformationen oder Fehler bei der Definition eines Schemas). | **25% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig, aber mit mittleren Mängeln**          | Der Code läuft und liefert teilweise korrekte Ergebnisse für einen Aspekt, aber wichtige Details fehlen (z. B. ungenaue Bounding Boxes, unvollständige Integration der extrahierten Daten). | **50% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig, aber mit minimalen Mängeln**          | Der Code erfüllt die Anforderungen eines Aspekts weitgehend, aber kleinere Fehler oder Abweichungen (z. B. nicht robust Extraktionsdaten, kleinere Schemaabweichungen, Prompt zu wenig stringent formuliert -> teilweise unstabile Output)sind vorhanden. | **75% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig und korrekt**                         | Der Code erfüllt die Anforderungen des Aspekts vollständig und liefert die erwarteten Ergebnisse ohne Fehler (z. B. korrekte Extraktion, vollständige Bounding Boxes, saubere Integration). | **100% der max. erreichbaren Punkte** |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675da2c",
   "metadata": {},
   "source": [
    "### A0. Setup (0 Punkte)\n",
    "\n",
    "Führen Sie diese Zellen aus, nachdem Sie die `.env` Datei mit Ihren API-Keys in den Codespace gezogen haben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1fb54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63121b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3d0429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "# === A0. Setup (Bitte Schlüssel nur lokal eintragen, nicht ins Versionskontrollsystem hochladen) ===\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# OpenAI-Client\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Deepinfra-Client (OpenAI-kompatible API)\n",
    "deepinfra_client = OpenAI(\n",
    "    api_key=deepinfra_api_key,\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f057e0",
   "metadata": {},
   "source": [
    "### A1. Funktion `ask_openai` (10 Punkte)\n",
    "\n",
    "Implementieren Sie eine Funktion `ask_openai`, die eine Anfrage mit Kontext an das OpenAI-Chat-API schickt und die Antwort als String zurückgibt.\n",
    "\n",
    "**Anforderungen:**\n",
    "1. Signatur:\n",
    "   ```python\n",
    "   def ask_openai(user_input: str, context: str) -> str:\n",
    "       ...\n",
    "   ```\n",
    "2. Verwenden Sie den bereits definierten `openai_client` aus A0.\n",
    "3. Nutzen Sie das Modell `\"gpt5-mini\"`.\n",
    "4. Die Funktion soll **nur den Textinhalt** der Antwort zurückgeben (ohne zusätzliche Prints).\n",
    "5. Der Kontext soll in den Systemprompt (injected) werden.\n",
    "\n",
    "Testen Sie Ihre Funktion danach mit **zwei Aufrufen**:\n",
    "- einmal mit dem Prompt: *\"Schreibe ein Haiku über Rekursion in der Programmierung.\"*\n",
    "- einmal mit dem Prompt: *\"Erkläre in einfachen Worten, was der Parameter `temperature` bei LLMs macht.\"*\n",
    "\n",
    "Drucken Sie beide Antworten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3770964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58a1cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = \"Schreibe ein Haiku über Rekursion in der Programmierung.\"\n",
    "\n",
    "context1 = \"\"\"\n",
    "Rekursion ist eine Technik in der Programmierung, bei der eine Funktion sich selbst aufruft, um ein Problem in kleinere Teilprobleme zu zerlegen. \n",
    "Typischerweise gibt es dabei eine sogenannte Basisbedingung, bei der die Funktion ohne weiteren Selbstaufruf endet. \n",
    "Dadurch lassen sich komplexe Strukturen wie Bäume oder verschachtelte Listen oft sehr elegant bearbeiten. \n",
    "Allerdings kann Rekursion auch zu Fehlern führen, wenn die Basisbedingung fehlt oder nie erreicht wird. \n",
    "In vielen Fällen gibt es sowohl eine rekursive als auch eine iterative Lösung, die sich in Verständlichkeit und Effizienz unterscheiden können.\n",
    "\"\"\"\n",
    "\n",
    "user_input2 = \"Erkläre in einfachen Worten, was der Parameter `temperature` bei LLMs macht.\"\n",
    "context2 = \"\"\"\n",
    "Der Parameter 'temperature' steuert bei vielen Sprachmodellen, wie kreativ oder deterministisch die Antworten sind. \n",
    "Bei einer niedrigen Temperatur (z.B. 0.1) wählt das Modell eher die wahrscheinlichsten Worte und verhält sich dadurch konservativ und vorhersehbar. \n",
    "Bei einer höheren Temperatur (z.B. 0.8 oder 1.0) werden auch unwahrscheinlichere Worte stärker berücksichtigt, was zu vielfältigeren und kreativeren Antworten führt. \n",
    "Zu hohe Werte können allerdings dazu führen, dass die Ausgabe unzusammenhängend oder unsinnig wird. \n",
    "Die Wahl einer passenden Temperatur hängt daher vom Anwendungsfall ab, etwa ob man eher präzise Fakten oder kreative Ideen haben möchte.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8b72287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(user_input: str, context: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    TODO (A1): Implementieren Sie diese Funktion.\n",
    "    Anforderungen siehe Aufgabenstellung oben.\n",
    "    \"\"\"\n",
    "    system_prompt = \"Um was geht es im Text?\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6ceaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two executions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a505f",
   "metadata": {},
   "source": [
    "### A2. Deepinfra-Aufruf korrigieren und erweitern (10 Punkte)\n",
    "\n",
    "Unten finden Sie einen (bewusst) fehlerhaften Beispielaufruf an ein Modell über die Deepinfra-API.  \n",
    "**Ihre Aufgaben:**\n",
    "\n",
    "1. **Korrigieren** Sie den Aufruf so, dass er syntaktisch korrekt ist und mit dem `deepinfra_client` funktioniert.  \n",
    "   - Achten Sie insbesondere auf Parameter-Namen und den Zugriff auf das Antwortobjekt.\n",
    "2. Erweitern Sie den Code so, dass **zwei Gesprächsrunden** mit dem Modell stattfinden:\n",
    "   - Erste User-Frage: *\"Nenne zwei unterschiedliche Möglichkeiten, wie Jugendliche sich für den Umweltschutz politisch engagieren können.\"*\n",
    "   - Zweite User-Frage: *\"Erläutere die zweite Möglichkeit etwas genauer.\"*\n",
    "   Verwenden Sie dafür die gleiche `messages`-Liste und fügen Sie nur die neue Nutzer-Nachricht und die Assistenten-Antwort hinzu.\n",
    "3. Implementieren Sie eine einfache **Längenbegrenzung**:  \n",
    "   Wenn die endgültige Antwort-Variable mehr als **400 Zeichen** enthält, kürzen Sie sie auf 400 Zeichen und hängen Sie `\"...\"` an.\n",
    "\n",
    "Verwenden Sie ein geeignetes deutschsprachiges oder multilinguales Modell aus Deepinfra (z.B. ein Llama- oder Mixtral-Chatmodell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "734b8844",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `BAAI/bge-small-en` does not exist', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mBAAI/bge-small-en\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Beispiel – ggf. anpassen\u001b[39;00m\n\u001b[32m      6\u001b[39m messages = [\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDu bist eine sachliche, gut informierte Assistenz.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      8\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNenne zwei unterschiedliche Möglichkeiten, wie Jugendliche sich für den Umweltschutz politisch engagieren können.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      9\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m chat_completion = \u001b[43mdeepinfra_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# TODO (A2):\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 1. Fehler korrigieren\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 2. Zweite Gesprächsrunde hinzufügen (weitere user-Nachricht + neue completion)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 3. Antwort ggf. auf 400 Zeichen kürzen und ausgeben\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `BAAI/bge-small-en` does not exist', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# A2. Deepinfra-Aufruf – fehlerhafte Ausgangsversion\n",
    "# TODO (A2): Korrigieren und erweitern Sie diesen Code.\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"  # Beispiel – ggf. anpassen\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Du bist eine sachliche, gut informierte Assistenz.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Nenne zwei unterschiedliche Möglichkeiten, wie Jugendliche sich für den Umweltschutz politisch engagieren können.\"},\n",
    "]\n",
    "\n",
    "chat_completion = deepinfra_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# TODO (A2):\n",
    "# 1. Fehler korrigieren\n",
    "# 2. Zweite Gesprächsrunde hinzufügen (weitere user-Nachricht + neue completion)\n",
    "# 3. Antwort ggf. auf 400 Zeichen kürzen und ausgeben\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afc978",
   "metadata": {},
   "source": [
    "### A3. Kurzfrage: Unterschiedliche Provider (5 Punkte)\n",
    "\n",
    "Beantworten Sie die folgende Frage in **3–4 Sätzen** in der Markdown-Zelle darunter:\n",
    "\n",
    "> Nennen Sie **einen Vorteil** der Nutzung eines gehosteten Dienstes wie OpenAI im Vergleich zu einem selbst gehosteten Modell,\n",
    "> und nennen Sie **mindestens einen Nachteil bzw. ein Risiko**.\n",
    "\n",
    "Gehen Sie dabei kurz auf Aspekte wie zum Beispiel Leistung, Kosten, Flexibilität oder Datenschutz ein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6406e",
   "metadata": {},
   "source": [
    "**Antwort A3:**\n",
    "\n",
    "Der Vorteil eines gehosteten Dienstes, ist dass dieses durch die unheimlichen Datenmengen schnell trainiert werden kann. Ein selbst gehostetes Modell hat potenziell nicht die Rechenleistung, um mit dem gleichen Training mitzuhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a50e10",
   "metadata": {},
   "source": [
    "Das Risiko bei einem gehosteten Dienst besteht darin, dass das Modell potenziell \"übertrianiert\" ist und beginnt zu interpretieren, wenn der Prompt nicht deutlich genug ist.\n",
    "Bei einem eigenen Dienst, kann man den Dienst spezifisch auf eine Aufgabe bereits trainieren, sodass das Modell keinen Raum zur Interpretation hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8117f",
   "metadata": {},
   "source": [
    "---\n",
    "## Teil B – Retrieval-Augmented Generation (RAG) (35 Punkte)\n",
    "\n",
    "In diesem Teil arbeiten Sie mit einer kleinen RAG-Pipeline.  \n",
    "Die zugrundeliegenden Texte behandeln das Thema **\"Jugend, Politik, Umwelt & Partizipation\"** in deutscher Sprache.\n",
    "\n",
    "Die folgenden Objekte werden in vorbereiteten Zellen erstellt:\n",
    "- `documents`: Liste von Rohtexten (z.B. aus PDF- oder Textdateien)\n",
    "- `chunks`: Liste von Textchunks\n",
    "- `embedder`: Objekt mit einer Methode `.encode(texts, convert_to_numpy=True)`\n",
    "- `index`: ein FAISS-Index, der Embeddings von `chunks` enthält\n",
    "\n",
    "Sie sollen nun die fehlenden Teile der Pipeline implementieren bzw. anpassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad34c",
   "metadata": {},
   "source": [
    "### B0. RAG-Setup (0 Punkte – bitte nicht verändern)\n",
    "\n",
    "Diese Zelle lädt die Dokumente, erzeugt Chunks, berechnet Embeddings und baut den FAISS-Index auf.  \n",
    "**Verändern Sie diese Zelle nicht.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:466\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    461\u001b[39m     message = (\n\u001b[32m    462\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 403 Forbidden: None.\nCannot access content at: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config.json.\nMake sure your token has the correct permissions.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is on.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhead_call_error\u001b[39;00m\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnzahl Chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 3. Embedder und Embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m embedder = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m embeddings = embedder.encode(chunks, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 4. FAISS-Index erstellen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:339\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    327\u001b[39m         modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m             model_name_or_path,\n\u001b[32m    329\u001b[39m             token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m             config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m         )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         modules = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[32m    353\u001b[39m     modules = OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:2112\u001b[39m, in \u001b[36mSentenceTransformer._load_auto_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[39m\n\u001b[32m   2109\u001b[39m tokenizer_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **tokenizer_kwargs}\n\u001b[32m   2110\u001b[39m config_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **config_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m2112\u001b[39m transformer_model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:87\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# plus some common values like \"input_ids\", \"attention_mask\", etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:162\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1332\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1330\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1334\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/utils/hub.py:553\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\u001b[39;00m\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    554\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWe couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to load the files, and couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find them in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m cached files.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCheck your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# later on anyway and re-raised if needed\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, HTTPError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n",
      "\u001b[31mOSError\u001b[39m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "# === B0. RAG-Setup (Bitte nicht verändern) ===\n",
    "import os\n",
    "from typing import List\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Dokumente Laden\n",
    "\n",
    "DATA_DIR = \"data/*.pdf\"  # Ordner mit Texten zu \"Jugend, Politik, Umwelt & Partizipation\"\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for pdf_path in glob.glob(DATA_DIR):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += \" \" + page_text\n",
    "\n",
    "# Split into chunks\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=250\n",
    ")\n",
    "\n",
    "chunks = splitter_a.split_text(text)\n",
    "\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "\n",
    "# 3. Embedder und Embeddings\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = embedder.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# 4. FAISS-Index erstellen\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"RAG-Setup abgeschlossen.\")\n",
    "\n",
    "\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index, \"faiss/faiss_exam.index\")\n",
    "with open(\"faiss/chunks_exam.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27faf433",
   "metadata": {},
   "source": [
    "### Run in case of crash later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde60375",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/third-party/faiss/faiss/impl/io.cpp:69: Error: 'f' failed: could not open faiss/faiss_exam.index for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_33089/1556424591.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m index = faiss.read_index(\u001b[33m\"faiss/faiss_exam.index\"\u001b[39m)\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m open(\u001b[33m\"faiss/chunks_exam.pkl\"\u001b[39m, \u001b[33m\"rb\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     chunks = pickle.load(f)\n",
      "\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/faiss/swigfaiss_avx2.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m  12522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m read_index(*args):\n\u001b[32m> \u001b[39m\u001b[32m12523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss_avx2.read_index(*args)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/third-party/faiss/faiss/impl/io.cpp:69: Error: 'f' failed: could not open faiss/faiss_exam.index for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss/faiss_exam.index\")\n",
    "\n",
    "with open(\"faiss/chunks_exam.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321272d",
   "metadata": {},
   "source": [
    "### B1. Funktion `retrieve_texts` korrigieren (10 Punkte)\n",
    "\n",
    "Die folgende Funktion soll zu einer Anfrage (`query`) die **Top-`k` ähnlichsten Chunks** zurückgeben.\n",
    "Die Implementierung ist fehlerhaft.\n",
    "\n",
    "**Ihre Aufgaben:**\n",
    "1. Korrigieren Sie die Funktion so, dass sie:\n",
    "   - die Query korrekt mit dem `embedder` embedden kann,\n",
    "   - den FAISS-Index `index` korrekt abfragt und\n",
    "   - eine **Liste von Strings** mit den gefundenen Chunks zurückgibt.\n",
    "2. Achten Sie insbesondere auf die richtige Form der Embedding-Arrays und den Umgang mit den Indizes.\n",
    "\n",
    "Verwenden Sie die bereits existierenden Objekte `index`, `chunks` und `embedder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93522671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def retrieve_texts(query: str, k: int, index, chunks: List[str], embedder) -> List[str]:\n",
    "    \"\"\"Gibt die Top-k ähnlichsten Chunks für eine Query zurück.\n",
    "\n",
    "    TODO (B1): Funktion korrigieren.\n",
    "    \"\"\"\n",
    "    # FEHLERHAFTE BEISPIELIMPLEMENTIERUNG:\n",
    "    query_emb = embedder.encode(query, convert_to_numpy=True)  # Form kann problematisch sein\n",
    "    distances, indices = index.search(query_emb, k)  # hier kann ein Fehler auftreten\n",
    "    return [chunks[indices]]  # ebenfalls fehlerhaft\n",
    "\n",
    "# Sie können die Funktion z.B. mit einer Test-Query ausprobieren:\n",
    "test_query = \"Wie können Jugendliche sich politisch für den Klimaschutz engagieren?\"\n",
    "try:\n",
    "    test_results = retrieve_texts(test_query, k=3, index=index, chunks=chunks, embedder=embedder)\n",
    "    for i, r in enumerate(test_results, start=1):\n",
    "        print(f\"Chunk {i} (Ausschnitt):\", r[:200], \"\\n---\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Fehler bei Testabfrage (erwartet, solange die Funktion noch nicht korrigiert ist):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251e5f3",
   "metadata": {},
   "source": [
    "### B2. RAG-Antwortfunktion `answer_query` anpassen (10 Punkte)\n",
    "\n",
    "Unten sehen Sie eine Funktion `answer_query`, die eine Nutzerfrage beantwortet, indem sie:\n",
    "1. mit `retrieve_texts` passende Chunks zum Thema findet,\n",
    "2. diese als Kontext an das LLM schickt und\n",
    "3. eine Antwort generiert.\n",
    "\n",
    "Die Funktion ist noch nicht korrekt auf das Thema **\"Jugend, Politik, Umwelt & Partizipation\"** und auf RAG zugeschnitten.\n",
    "\n",
    "**Ihre Aufgaben:**\n",
    "1. Passen Sie den `system_prompt` an die neue Domäne an und geben sie klare Anweisungen wie mit dem Kontext umzugehen ist.  \n",
    "3. Verwenden Sie das Modell `\"gpt-5-mini\"` über den `openai_client` aus Teil A.\n",
    "4. Wählen Sie einen sinnvollen Standardwert für `k`und kommentieren Sie in **einem kurzen Kommentar im Code**, warum diese Wahl für diesen Anwendungsfall plausibel ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ad253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, k: int, index, chunks: List[str], embedder, client: OpenAI) -> str:\n",
    "    \"\"\"Beantwortet eine Nutzerfrage mittels RAG.\n",
    "\n",
    "    TODO (B2):\n",
    "    - Prompt anpassen\n",
    "    - Sicherstellen, dass nur Kontext verwendet wird\n",
    "    - openai_client und gpt-5-mini nutzen\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Du bist eine hilfreiche Assistenz, die Fragen zu MEDIZINISCHEN LEITLINIEN beantwortet. \"\n",
    "        \"(TODO: Dies ist noch nicht an die neue Domäne angepasst.)\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Kontext:\\n{context}\\n\\nFrage: {query}\"},\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Beispielaufruf (wenn B1 und B2 korrekt implementiert sind):\n",
    "example_query = \"Welche Formen politischer Beteiligung von Jugendlichen im Umweltschutz werden im Text genannt?\"\n",
    "try:\n",
    "    answer = answer_query(example_query, k=, index=index, chunks=chunks, embedder=embedder, client=openai_client)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(\"Fehler bei Beispielaufruf (erwartet, solange B1/B2 noch nicht fertig sind):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc5bb2",
   "metadata": {},
   "source": [
    "### B3. Einfache Retrieval-Sanity-Checks (5 Punkte)\n",
    "\n",
    "Erweitern Sie die Funktion `answer_query` aus B2 wie folgt:\n",
    "\n",
    "1. **Vor dem LLM-Aufruf** soll ein kurzer Ausschnitt (ca. die ersten 200 Zeichen) des **besten (Top-1) Chunks** ausgegeben werden, z.B. mit `print(\"Top-1 Chunk:\", ...)`.\n",
    "2. Falls aus irgendeinem Grund **keine Chunks** zurückgegeben werden (z.B. leere Liste), soll die Funktion **keinen LLM-Aufruf** machen, sondern einen verständlichen Hinweis-String zurückgeben, z.B.:\n",
    "   > \"Es konnten keine passenden Textstellen gefunden werden.\"\n",
    "\n",
    "Implementieren Sie diese Änderungen direkt in der Funktion `answer_query` oben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65034c5",
   "metadata": {},
   "source": [
    "_Hinweis: Sie müssen hier keine neue Zelle schreiben. Passen Sie die bestehende Implementierung von `answer_query` direkt an._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87068cb8",
   "metadata": {},
   "source": [
    "### B4. Fragen und Kriterien für eine Evaluation des RAG-Systems (10 Punkte)\n",
    "\n",
    "Sie haben in diesem Notebook ein RAG-System aufgebaut, das Fragen zum Thema\n",
    "**„Jugend, Politik, Umwelt & Partizipation“** beantwortet.\n",
    "\n",
    "In dieser Aufgabe sollen Sie das LLM nutzen, um:\n",
    "\n",
    "1. sinnvolle **Evaluationsfragen** zu generieren, mit denen man das System testen könnte, und\n",
    "2. **Kriterien** vorschlagen zu lassen, auf die man bei der Evaluation achten sollte.\n",
    "\n",
    "---\n",
    "\n",
    "#### B4.1 – Funktion `generate_eval_questions` implementieren\n",
    "\n",
    "Implementieren Sie eine Funktion, die mit Hilfe des OpenAI-Clients _**10**_ Evaluationsfragen generiert.\n",
    "\n",
    "**Anforderungen:**\n",
    "\n",
    "1. Signatur:\n",
    "   ```python\n",
    "   def generate_eval_questions(chunks, num_chunks: int = 5) -> str:\n",
    "       ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_questions(chunks, num_chunks) -> list:\n",
    "\n",
    "    # B4.2 – Kriterien für die Auswertung generieren\n",
    "\n",
    "    # TODO (B4.2):\n",
    "    # - eigenen Systemprompt definieren (Evaluationsexpert*in) \n",
    "    # - jeden chunk aus dem sample in den prompt injecten\n",
    "    # - jede generierte Frage in die Liste einfügen\n",
    "\n",
    "    questions = []\n",
    "    prompt_eval = (\n",
    "        \"TODO: Formulieren Sie hier eine Anfrage, in der Sie das Modell bitten, \"\n",
    "        \"Fragen zu genierieren, die zur Evaluation eines \"\n",
    "        \"RAG-Systems (Jugend, Politik, Umwelt & Partizipation) achten sollte. \" \n",
    "        \"Es soll genau eine Frage, und nur die Frage ausgegeben werden\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ea877",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_eval_questions()\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95190cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Ende der Prüfung\n",
    "\n",
    "Überprüfen Sie, ob alle Code-Zellen ausgeführt wurden und alle Antworten (auch die theoretischen Fragen) eingetragen sind.\n",
    "\n",
    "Viel Erfolg!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2b83b",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('IP Address:', socket.gethostbyname(socket.gethostname()))\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
