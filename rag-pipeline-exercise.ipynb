{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad32605",
   "metadata": {},
   "source": [
    "# RAG Pipeline Exercise\n",
    "\n",
    "In this exercise you will build and **compare two simple Retrieval-Augmented Generation (RAG) pipelines**.\n",
    "\n",
    "You will work with a small collection of PDF documents (e.g. medical guidelines) and:\n",
    "\n",
    "1. Load and chunk the PDF documents.\n",
    "2. Create a vector index using **embedding model A** (local `BAAI/bge-m3`).\n",
    "3. Create a second index using **embedding model B** (e.g. OpenAI or Gemini embeddings).\n",
    "4. Implement a simple **retriever** and an **answering function** that calls an LLM with retrieved context.\n",
    "5. Automatically **generate questions** from the documents and use them to **compare two RAG configurations**.\n",
    "\n",
    "Cells marked with `# TODO` are **for students to implement**.\n",
    "Everything else is provided scaffolding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf82e6",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be93ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f16980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env (you need to create this file once and add your keys)\n",
    "load_dotenv()\n",
    "\n",
    "deepinfra_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2719d",
   "metadata": {},
   "source": [
    "## 1. Load PDF documents\n",
    "\n",
    "We assume there is a `data/` folder containing one or more PDF files.\n",
    "\n",
    "**Task:** implement `load_pdfs(glob_path)` so that it:\n",
    "- Iterates over all PDF files matching `glob_path`\n",
    "- Reads them with `PdfReader`\n",
    "- Concatenates the text of all pages into **one long string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8abcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416c3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 230708\n",
      "Preview: Asthma: diagnosis, \n",
      "moni toring and chr onic \n",
      "asthma manag emen t (BTS, \n",
      "NICE, SI GN) \n",
      "NICE guideline \n",
      "Published: 27 No vember 202 4 \n",
      "www .nice.or g.uk/guidance/ng2 45 \n",
      "© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\n",
      "conditions#notice-of -right s).\n",
      "Your r esponsi bility \n",
      "The r ecommendations in t his guideline r epresent t he view of NICE, arriv ed at aft er car eful \n",
      "consideration of t he evidence a vailable. When e xercising t heir judge\n"
     ]
    }
   ],
   "source": [
    "# Run once and inspect\n",
    "raw_text = load_pdfs(\"data/*.pdf\")\n",
    "print(\"Number of characters:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5a14",
   "metadata": {},
   "source": [
    "## 2. Chunk the text\n",
    "\n",
    "We will split the long text into overlapping chunks.\n",
    "\n",
    "Later you can **experiment** with different `chunk_size` and `chunk_overlap` to see how it affects retrieval.\n",
    "\n",
    "**Task:** start with the given parameters, run once, then try at least one alternative configuration and note the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e3f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG A: 130 chunks produced, first chunk length = 1995\n",
      "RAG B: 260 chunks produced, first chunk length = 979\n"
     ]
    }
   ],
   "source": [
    "# Base configuration (RAG A)\n",
    "chunk_size_a = 2000\n",
    "chunk_overlap_a = 200\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_a,\n",
    "    chunk_overlap=chunk_overlap_a\n",
    ")\n",
    "\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "print(f\"RAG A: {len(chunks_a)} chunks produced, first chunk length = {len(chunks_a[0])}\")\n",
    "\n",
    "# TODO (mini-experiment): change chunk_size / chunk_overlap for RAG B and compare\n",
    "chunk_size_b =  1000\n",
    "chunk_overlap_b = 100\n",
    "\n",
    "splitter_b = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_b,\n",
    "    chunk_overlap=chunk_overlap_b\n",
    ")\n",
    "\n",
    "chunks_b = splitter_b.split_text(raw_text)\n",
    "print(f\"RAG B: {len(chunks_b)} chunks produced, first chunk length = {len(chunks_b[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3511f4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings and a FAISS index\n",
    "\n",
    "We start with **Embedding model A: `BAAI/bge-small-en`** using `sentence-transformers`. You can find a list of more models here: https://huggingface.co/spaces/mteb/leaderboard \n",
    "make sure that the models are not bigger than the one used here. Otherwise the embeddings process will take too long.\n",
    "\n",
    "Then, as an optional extension, you can build **Embedding model B** using OpenAI or Gemini and compare.\n",
    "\n",
    "To keep the exercise manageable, the base version only **requires** BGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b519161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensionality (A): 384\n",
      "FAISS index (A) size: 130\n"
     ]
    }
   ],
   "source": [
    "# Embedding model A (local)\n",
    "model_name_a = \"BAAI/bge-small-en\"\n",
    "embedder_a = SentenceTransformer(model_name_a)\n",
    "\n",
    "# Compute embeddings for all chunks of configuration A\n",
    "embeddings_a = embedder_a.encode(chunks_a, convert_to_numpy=True)\n",
    "\n",
    "dimensions_a = embeddings_a.shape[1]\n",
    "print(\"Embedding dimensionality (A):\", dimensions_a)\n",
    "\n",
    "index_a = faiss.IndexFlatL2(dimensions_a)\n",
    "index_a.add(embeddings_a)\n",
    "print(\"FAISS index (A) size:\", index_a.ntotal)\n",
    "\n",
    "# Persist index/chunks if you like (optional)\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index_a, \"faiss/faiss_index_a.index\")\n",
    "with open(\"faiss/chunks_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77271916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index (B) size: 260\n"
     ]
    }
   ],
   "source": [
    "# Embedding model B using OpenAI embeddings.\n",
    "\n",
    "# TODO :\n",
    "# - Use `openai_client.embeddings.create(...)` to compute embeddings for `chunks_b`\n",
    "# - Create a second FAISS index `index_b`\n",
    "# - Make sure to check the dimensionality from the first embedding vector\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "response = openai_client.embeddings.create(\n",
    "     model=\"text-embedding-3-small\",\n",
    "    input=chunks_b\n",
    ")\n",
    "embeddings_b = np.array([item.embedding for item in response.data])\n",
    "dim_b = embeddings_b.shape[1]\n",
    "index_b = faiss.IndexFlatL2(dim_b)\n",
    "index_b.add(embeddings_b)\n",
    "print(\"FAISS index (B) size:\", index_b.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339224e",
   "metadata": {},
   "source": [
    "## 4. Implement a simple retriever\n",
    "\n",
    "We now implement a generic retrieval function that:\n",
    "1. Embeds the query.\n",
    "2. Searches the FAISS index.\n",
    "3. Returns the corresponding text chunks.\n",
    "\n",
    "We implement it for configuration A. If you built configuration B, you can reuse the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6fbf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved chunks: 5\n",
      "Preview of first chunk: and signs of ot her causes of r espirat ory sympt oms but be awar e that e ven if \n",
      "examination r esult s are normal, t he person ma y still ha ve ast hma. [NICE 2017] \n",
      "Initial tr eatmen t and obje cti\n"
     ]
    }
   ],
   "source": [
    "def retrieve_texts(query: str, k: int, index, chunks, embedder) -> list:\n",
    "    \"\"\"Return the top-k most similar chunks for a query.\n",
    "    - Encode the query with `embedder.encode(...)`\n",
    "    - Call `index.search(query_embedding, k)`\n",
    "    - Use the returned indices to select the chunks\n",
    "    - Return a list of strings (chunks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return retrieved\n",
    "\n",
    "# Quick sanity check\n",
    "test_query = \"What is the most important factor in diagnosing asthma?\"\n",
    "retrieved_test = retrieve_texts(\n",
    "    query = test_query,\n",
    "    k=5,\n",
    "    index = index_a,\n",
    "    chunks = chunks_a,\n",
    "    embedder = embedder_a\n",
    ")\n",
    "print(\"Number of retrieved chunks:\",len(retrieved_test))\n",
    "print(\"Preview of first chunk:\", retrieved_test[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646fdc6",
   "metadata": {},
   "source": [
    "## 5. Implement `answer_query` using an LLM\n",
    "\n",
    "Now we build the actual RAG call:\n",
    "\n",
    "1. Use `retrieve_texts` to get top-`k` chunks.\n",
    "2. Concatenate them into a context string.\n",
    "3. Build a prompt that:\n",
    "   - shows the context\n",
    "   - asks the model to answer the user question based **only** on this context.\n",
    "4. Call the OpenAI chat completion API.\n",
    "\n",
    "This is the **core RAG function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d94610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG answer: Of course! What question do you have?\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query: str, k: int, index, chunks, embedder, client: OpenAI) -> str:\n",
    "    \"\"\"RAG-style answer: retrieve context and ask an LLM.\n",
    "\n",
    "    TODO (students):\n",
    "    - Use `retrieve_texts` to get `k` relevant chunks.\n",
    "    - Join them into a single context string.\n",
    "    - Build a chat prompt that instructs the model to answer *only* using the context.\n",
    "    - Call `client.chat.completions.create(...)` with model `\"gpt-5o-mini\"` (or similar).\n",
    "    - Return the model's answer text.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Remember: strings can be concatenated (like an addition)\n",
    "    system_prompt = (\n",
    "        \"You are a helpful Assistant. Answer the users question.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Quick manual test\n",
    "answer = answer_query(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"RAG answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86a92e",
   "metadata": {},
   "source": [
    "## 6. Generate questions from random chunks (automatic evaluation set)\n",
    "\n",
    "To compare two RAG configurations, we need **questions**.\n",
    "\n",
    "We will:\n",
    "- randomly sample a few chunks from the corpus,\n",
    "- ask an LLM to generate a **good question** whose answer is contained in the chunk.\n",
    "\n",
    "Then we can use these question–chunk pairs as a small evaluation set.\n",
    "\n",
    "We provide most of the implementation. Your job is mainly to:\n",
    "- inspect the code,\n",
    "- understand the prompt,\n",
    "- maybe tweak the number of chunks or retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb899fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What implications do the recommendations for asthma management in children aged 5 to 11, particularly regarding the use of regular low-dose ICS, have for current treatment practices and future research directions?\n",
      "  From chunk preview: applies her e. The r ecommendations ar e not based on a specific e vidence sear ch, but t he \n",
      "committ ee not ed that peo...\n",
      "\n",
      "Q2: What evidence led the committee to recommend as-needed combination inhalers over standard SABA treatments for the management of newly diagnosed asthma in individuals aged 12 and over?\n",
      "  From chunk preview: How the r ecommenda tion mig ht affect practice \n",
      "Digital inhalers ar e not r ecommended f or routine use in t he NHS, an...\n",
      "\n",
      "Q3: What were the main considerations and decisions made by the committee regarding the use of beta-blockers and the recommendations for step 1 antihypertensive treatment in individuals with type 2 diabetes, particularly for those of Black African or African-Caribbean family origin?\n",
      "  From chunk preview: that t he pr evious r ecommendations f or st ep 1 treatment should be r etained (wit h minor \n",
      "changes f or clarity), bec...\n",
      "\n",
      "Q4: What are the recommended blood pressure targets for individuals under 80 with hypertension or related conditions, according to the 2019 NICE guidelines?\n",
      "  From chunk preview: of the long-t erm balance of tr eatment benefit and risks. [2019] \n",
      "For a shor t explanation of wh y the committ ee made ...\n",
      "\n",
      "Q5: What considerations did the committee take into account when revising the recommendations for antihypertensive therapy in individuals of Black African or African-Caribbean descent with type 2 diabetes, and how might these changes impact clinical practice?\n",
      "  From chunk preview: clinically equiv alent. \n",
      "For people of Black African or African–Caribbean f amily origin wit h type  2 diabet es, the \n",
      "p...\n",
      "\n",
      "Q6: What steps should be taken to assess and diagnose postural hypotension and hypertension in older patients according to NICE guidelines?\n",
      "  From chunk preview: people, see NICE's guideline on f alls in older people: assessing risk and \n",
      "prevention ) \n",
      "• measur e subsequent blood pr...\n",
      "\n",
      "Q7: What are the challenges and implications of implementing the recommended diagnostic tests for asthma in clinical practice, especially in relation to their availability and the potential impact on healthcare investment and diagnosis accuracy?\n",
      "  From chunk preview: out in curr ent practice, wit h the exception of spir ometr y and r eversibility t esting, which is \n",
      "performed in some a...\n",
      "\n",
      "Q8: How should healthcare commissioners and providers align their strategies for managing hypertension with their responsibilities to eliminate discrimination, advance equality, and promote environmental sustainability?\n",
      "  From chunk preview: use it. The y should do so in t he cont ext of local and national priorities f or funding and \n",
      "developing ser vices, and...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_questions_for_random_chunks(chunks, num_chunks: int = 5, max_retries: int = 2):\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    qa_pairs = []\n",
    "\n",
    "    for chunk in selected_chunks:\n",
    "        prompt = prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "\n",
    "        question = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                question = completion.choices[0].message.content.strip()\n",
    "                if question:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating question, retrying...\", e)\n",
    "\n",
    "        if question is None:\n",
    "            question = \"Error: could not generate question.\"\n",
    "\n",
    "        qa_pairs.append((chunk, question))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "questions = generate_questions_for_random_chunks(chunks_a, num_chunks=8, max_retries=2)\n",
    "for i, (chunk, q) in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q}\\n  From chunk preview: {chunk[:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0eaf5",
   "metadata": {},
   "source": [
    "## 7. Compare two RAG configurations\n",
    "\n",
    "Now we can:\n",
    "- Use the generated questions,\n",
    "- Answer them with RAG configuration A (BGE + chunking A),\n",
    "- (Optional) Answer them with RAG configuration B (e.g. different chunking and/or different embeddings),\n",
    "- Compare the answers qualitatively.\n",
    "\n",
    "To keep the exercise manageable, we start with config A only.\n",
    "If you implemented config B, reuse `answer_query` with `index_b`, `chunks_b`, and your second embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a292474a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     45\u001b[39m results_b = answer_generated_questions(\n\u001b[32m     46\u001b[39m     questions,\n\u001b[32m     47\u001b[39m     k=\u001b[32m5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     client=openai_client,\n\u001b[32m     52\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results_a:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mitem_a\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer A:\u001b[39m\u001b[33m\"\u001b[39m, item_a[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer B:\u001b[39m\u001b[33m\"\u001b[39m, item_b[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'item_a' is not defined"
     ]
    }
   ],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, chunks, embedder, client):\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        answer = answer_query(question, k, index, chunks, embedder, client)\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results_a = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_a,\n",
    "    chunks=chunks_a,\n",
    "    embedder=embedder_a,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Embedder B: Wrapper for OpenAI embeddings\n",
    "# Provides an .encode() method compatible with retrieve_texts()\n",
    "# ============================================================\n",
    "\n",
    "class OpenAIEmbedder:\n",
    "    def __init__(self, client, model=\"text-embedding-3-small\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def encode(self, texts, convert_to_numpy=True):\n",
    "        # Call OpenAI embeddings\n",
    "        resp = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=texts\n",
    "        )\n",
    "        # Convert embeddings to numpy array\n",
    "        vectors = np.array([item.embedding for item in resp.data])\n",
    "        return vectors\n",
    "\n",
    "\n",
    "# instantiate embedder_b\n",
    "embedder_b = OpenAIEmbedder(openai_client)\n",
    "\n",
    "results_b = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_b,\n",
    "    chunks=chunks_b,\n",
    "    embedder=embedder_b,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "\n",
    "for item in results_a:\n",
    "    print(\"Question:\", item_a[\"question\"])\n",
    "    print(\"Answer A:\", item_a[\"answer\"])\n",
    "    print(\"Answer B:\", item_b[\"answer\"])\n",
    "    print(\"Source chunk preview:\", item_a[\"chunk\"][:150], \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec212b7",
   "metadata": {},
   "source": [
    "Add RAG B and create a comparison table\n",
    "\n",
    "If you implemented a second configuration (e.g. different chunking + OpenAI embeddings):\n",
    "\n",
    "1. Build `index_b` and `embedder_b`.\n",
    "2. Run `results_b = answer_generated_questions(..., index_b, chunks_b, embedder_b, client)`.\n",
    "3. For each question, compare:\n",
    "   - Which answer is more complete / specific?\n",
    "   - Which one is better grounded in the source chunk?\n",
    "4. Summarise your findings in a short **markdown cell** or a small table.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the core RAG exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
