{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad32605",
   "metadata": {},
   "source": [
    "# RAG Pipeline Exercise\n",
    "\n",
    "In this exercise you will build and **compare two simple Retrieval-Augmented Generation (RAG) pipelines**.\n",
    "\n",
    "You will work with a small collection of PDF documents (e.g. medical guidelines) and:\n",
    "\n",
    "1. Load and chunk the PDF documents.\n",
    "2. Create a vector index using **embedding model A** (local `BAAI/bge-m3`).\n",
    "3. Create a second index using **embedding model B** (e.g. OpenAI or Gemini embeddings).\n",
    "4. Implement a simple **retriever** and an **answering function** that calls an LLM with retrieved context.\n",
    "5. Automatically **generate questions** from the documents and use them to **compare two RAG configurations**.\n",
    "\n",
    "Cells marked with `# TODO` are **for students to implement**.\n",
    "Everything else is provided scaffolding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf82e6",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be93ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f16980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env (you need to create this file once and add your keys)\n",
    "load_dotenv()\n",
    "\n",
    "deepinfra_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2719d",
   "metadata": {},
   "source": [
    "## 1. Load PDF documents\n",
    "\n",
    "We assume there is a `data/` folder containing one or more PDF files.\n",
    "\n",
    "**Task:** implement `load_pdfs(glob_path)` so that it:\n",
    "- Iterates over all PDF files matching `glob_path`\n",
    "- Reads them with `PdfReader`\n",
    "- Concatenates the text of all pages into **one long string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8abcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416c3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 230708\n",
      "Preview: Asthma: diagnosis, \n",
      "moni toring and chr onic \n",
      "asthma manag emen t (BTS, \n",
      "NICE, SI GN) \n",
      "NICE guideline \n",
      "Published: 27 No vember 202 4 \n",
      "www .nice.or g.uk/guidance/ng2 45 \n",
      "© NICE 202 4. All right s reserved. Subject t o Notice of right s (https://www .nice.or g.uk/t erms-and-\n",
      "conditions#notice-of -right s).\n",
      "Your r esponsi bility \n",
      "The r ecommendations in t his guideline r epresent t he view of NICE, arriv ed at aft er car eful \n",
      "consideration of t he evidence a vailable. When e xercising t heir judge\n"
     ]
    }
   ],
   "source": [
    "# Run once and inspect\n",
    "raw_text = load_pdfs(\"data/*.pdf\")\n",
    "print(\"Number of characters:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5a14",
   "metadata": {},
   "source": [
    "## 2. Chunk the text\n",
    "\n",
    "We will split the long text into overlapping chunks.\n",
    "\n",
    "Later you can **experiment** with different `chunk_size` and `chunk_overlap` to see how it affects retrieval.\n",
    "\n",
    "**Task:** start with the given parameters, run once, then try at least one alternative configuration and note the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e3f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG A: 130 chunks produced, first chunk length = 1995\n",
      "RAG B: 260 chunks produced, first chunk length = 979\n"
     ]
    }
   ],
   "source": [
    "# Base configuration (RAG A)\n",
    "chunk_size_a = 2000\n",
    "chunk_overlap_a = 200\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_a,\n",
    "    chunk_overlap=chunk_overlap_a\n",
    ")\n",
    "\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "print(f\"RAG A: {len(chunks_a)} chunks produced, first chunk length = {len(chunks_a[0])}\")\n",
    "\n",
    "# TODO (mini-experiment): change chunk_size / chunk_overlap for RAG B and compare\n",
    "chunk_size_b =  1000\n",
    "chunk_overlap_b = 100\n",
    "\n",
    "splitter_b = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_b,\n",
    "    chunk_overlap=chunk_overlap_b\n",
    ")\n",
    "\n",
    "chunks_b = splitter_b.split_text(raw_text)\n",
    "print(f\"RAG B: {len(chunks_b)} chunks produced, first chunk length = {len(chunks_b[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3511f4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings and a FAISS index\n",
    "\n",
    "We start with **Embedding model A: `BAAI/bge-small-en`** using `sentence-transformers`. You can find a list of more models here: https://huggingface.co/spaces/mteb/leaderboard \n",
    "make sure that the models are not bigger than the one used here. Otherwise the embeddings process will take too long.\n",
    "\n",
    "Then, as an optional extension, you can build **Embedding model B** using OpenAI or Gemini and compare.\n",
    "\n",
    "To keep the exercise manageable, the base version only **requires** BGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b519161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensionality (A): 384\n",
      "FAISS index (A) size: 130\n"
     ]
    }
   ],
   "source": [
    "# Embedding model A (local)\n",
    "model_name_a = \"BAAI/bge-small-en\"\n",
    "embedder_a = SentenceTransformer(model_name_a)\n",
    "\n",
    "# Compute embeddings for all chunks of configuration A\n",
    "embeddings_a = embedder_a.encode(chunks_a, convert_to_numpy=True)\n",
    "\n",
    "dimensions_a = embeddings_a.shape[1]\n",
    "print(\"Embedding dimensionality (A):\", dimensions_a)\n",
    "\n",
    "index_a = faiss.IndexFlatL2(dimensions_a)\n",
    "index_a.add(embeddings_a)\n",
    "print(\"FAISS index (A) size:\", index_a.ntotal)\n",
    "\n",
    "# Persist index/chunks if you like (optional)\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index_a, \"faiss/faiss_index_a.index\")\n",
    "with open(\"faiss/chunks_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77271916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index (B) size: 260\n"
     ]
    }
   ],
   "source": [
    "# Embedding model B using OpenAI embeddings.\n",
    "\n",
    "# TODO :\n",
    "# - Use `openai_client.embeddings.create(...)` to compute embeddings for `chunks_b`\n",
    "# - Create a second FAISS index `index_b`\n",
    "# - Make sure to check the dimensionality from the first embedding vector\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "response = openai_client.embeddings.create(\n",
    "     model=\"text-embedding-3-small\",\n",
    "    input=chunks_b\n",
    ")\n",
    "embeddings_b = np.array([item.embedding for item in response.data])\n",
    "dim_b = embeddings_b.shape[1]\n",
    "index_b = faiss.IndexFlatL2(dim_b)\n",
    "index_b.add(embeddings_b)\n",
    "print(\"FAISS index (B) size:\", index_b.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339224e",
   "metadata": {},
   "source": [
    "## 4. Implement a simple retriever\n",
    "\n",
    "We now implement a generic retrieval function that:\n",
    "1. Embeds the query.\n",
    "2. Searches the FAISS index.\n",
    "3. Returns the corresponding text chunks.\n",
    "\n",
    "We implement it for configuration A. If you built configuration B, you can reuse the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6fbf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved chunks: 5\n",
      "Preview of first chunk: and signs of ot her causes of r espirat ory sympt oms but be awar e that e ven if \n",
      "examination r esult s are normal, t he person ma y still ha ve ast hma. [NICE 2017] \n",
      "Initial tr eatmen t and obje cti\n"
     ]
    }
   ],
   "source": [
    "def retrieve_texts(query: str, k: int, index, chunks, embedder) -> list:\n",
    "    \"\"\"Return the top-k most similar chunks for a query.\n",
    "    - Encode the query with `embedder.encode(...)`\n",
    "    - Call `index.search(query_embedding, k)`\n",
    "    - Use the returned indices to select the chunks\n",
    "    - Return a list of strings (chunks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return retrieved\n",
    "\n",
    "# Quick sanity check\n",
    "test_query = \"What is the most important factor in diagnosing asthma?\"\n",
    "retrieved_test = retrieve_texts(\n",
    "    query = test_query,\n",
    "    k=5,\n",
    "    index = index_a,\n",
    "    chunks = chunks_a,\n",
    "    embedder = embedder_a\n",
    ")\n",
    "print(\"Number of retrieved chunks:\",len(retrieved_test))\n",
    "print(\"Preview of first chunk:\", retrieved_test[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646fdc6",
   "metadata": {},
   "source": [
    "## 5. Implement `answer_query` using an LLM\n",
    "\n",
    "Now we build the actual RAG call:\n",
    "\n",
    "1. Use `retrieve_texts` to get top-`k` chunks.\n",
    "2. Concatenate them into a context string.\n",
    "3. Build a prompt that:\n",
    "   - shows the context\n",
    "   - asks the model to answer the user question based **only** on this context.\n",
    "4. Call the OpenAI chat completion API.\n",
    "\n",
    "This is the **core RAG function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94610c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmessages = [\u001b[39m\n                ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query: str, k: int, index, chunks, embedder, client: OpenAI) -> str:\n",
    "    \"\"\"RAG-style answer: retrieve context and ask an LLM.\n",
    "\n",
    "    - Use `retrieve_texts` to get `k` relevant chunks.\n",
    "    - Join them into a single context string.\n",
    "    - Build a chat prompt that instructs the model to answer *only* using the context.\n",
    "    - Call `client.chat.completions.create(...)`.\n",
    "    - Return the model's answer text.\n",
    "    \"\"\"\n",
    "    # 1) Kontext holen\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # 2) System-Prompt mit Kontext\n",
    "    system_prompt = (\n",
    "        \"You are a retrieval-augmented assistant. \"\n",
    "        \"Answer the user question strictly and exclusively using the provided context. \"\n",
    "        \"If the answer is not in the context, reply: 'I don't know'.\\n\\n\"\n",
    "        f\"Context:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    # 3) Nachrichten für den Chat-Aufruf\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    # 4) Modell aufrufen\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # oder \"gpt-5o-mini\" je nach Vorgabe\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    # 5) Antwort zurückgeben\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Quick manual test\n",
    "answer = answer_query(\n",
    "    test_query,\n",
    "    k=3,\n",
    "    index=index_a,\n",
    "    chunks=chunks_a,\n",
    "    embedder=embedder_a,\n",
    "    client=openai_client,\n",
    ")\n",
    "print(\"RAG answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86a92e",
   "metadata": {},
   "source": [
    "## 6. Generate questions from random chunks (automatic evaluation set)\n",
    "\n",
    "To compare two RAG configurations, we need **questions**.\n",
    "\n",
    "We will:\n",
    "- randomly sample a few chunks from the corpus,\n",
    "- ask an LLM to generate a **good question** whose answer is contained in the chunk.\n",
    "\n",
    "Then we can use these question–chunk pairs as a small evaluation set.\n",
    "\n",
    "We provide most of the implementation. Your job is mainly to:\n",
    "- inspect the code,\n",
    "- understand the prompt,\n",
    "- maybe tweak the number of chunks or retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb899fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are the key criteria for identifying uncontrolled asthma, and how do these criteria potentially influence the management strategies recommended in the guidelines?\n",
      "  From chunk preview: A healt hcare professional wit h higher training in r espirat ory medicine and pr oficiency in \n",
      "the management of ast hm...\n",
      "\n",
      "Q2: What are the key recommendations for treating and monitoring hypertension according to the guidelines, and how do they address the need for specialist referrals and research contributions?\n",
      "  From chunk preview: 1.4 Treating and monit oring h yper tension ...............................................................................\n",
      "\n",
      "Q3: What implications do the updated recommendations for managing hypertension in individuals with type 2 diabetes of Black African or African-Caribbean descent have for clinical practice, particularly regarding the transition from monotherapy to dual therapy?\n",
      "  From chunk preview: clinically equiv alent. \n",
      "For people of Black African or African–Caribbean f amily origin wit h type  2 diabet es, the \n",
      "p...\n",
      "\n",
      "Q4: What alternatives does the committee suggest for diagnosing asthma in cases where standard tests like spirometry and FeNO are delayed or unavailable, particularly for children?\n",
      "  From chunk preview: The committ ee w ere awar e that t here can be dela ys in accessing spir ometr y and F eNO \n",
      "testing, and it is hoped t h...\n",
      "\n",
      "Q5: What steps should be taken if a patient’s asthma remains uncontrolled despite treatment with a moderate-dose MART regimen, and what factors should be assessed to guide further management?\n",
      "  From chunk preview: If treatment wit h MART using a lo w-dose maint enance r egimen does not pr ovide adequat e \n",
      "asthma contr ol, the commit...\n",
      "\n",
      "Q6: How can commissioners and providers effectively incorporate environmental sustainability into the implementation of NICE recommendations for asthma diagnosis and management?\n",
      "  From chunk preview: Commissioners and pr oviders ha ve a responsibility t o promot e an en vironmentally \n",
      "sustainable healt h and car e syst...\n",
      "\n",
      "Q7: What are the key considerations outlined in the new asthma management guideline for children aged 5 to 11, particularly regarding medication combinations, self-management strategies, and risk-stratified care?\n",
      "  From chunk preview: Medicine combination and sequencing in childr en aged 5 t o 11 ...................................................... 56...\n",
      "\n",
      "Q8: What was the rationale behind the committee's decision to not retain the previous recommendation for beta-blockers as step 1 antihypertensive treatment, and how has the management of blood pressure for adults with type 2 diabetes been updated in the new guidelines?\n",
      "  From chunk preview: that t he pr evious r ecommendations f or st ep 1 treatment should be r etained (wit h minor \n",
      "changes f or clarity), bec...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_questions_for_random_chunks(chunks, num_chunks: int = 5, max_retries: int = 2):\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    qa_pairs = []\n",
    "\n",
    "    for chunk in selected_chunks:\n",
    "        prompt = prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "\n",
    "        question = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                question = completion.choices[0].message.content.strip()\n",
    "                if question:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating question, retrying...\", e)\n",
    "\n",
    "        if question is None:\n",
    "            question = \"Error: could not generate question.\"\n",
    "\n",
    "        qa_pairs.append((chunk, question))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "questions = generate_questions_for_random_chunks(chunks_a, num_chunks=8, max_retries=2)\n",
    "for i, (chunk, q) in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q}\\n  From chunk preview: {chunk[:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0eaf5",
   "metadata": {},
   "source": [
    "## 7. Compare two RAG configurations\n",
    "\n",
    "Now we can:\n",
    "- Use the generated questions,\n",
    "- Answer them with RAG configuration A (BGE + chunking A),\n",
    "- (Optional) Answer them with RAG configuration B (e.g. different chunking and/or different embeddings),\n",
    "- Compare the answers qualitatively.\n",
    "\n",
    "To keep the exercise manageable, we start with config A only.\n",
    "If you implemented config B, reuse `answer_query` with `index_b`, `chunks_b`, and your second embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a292474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the key criteria for identifying uncontrolled asthma, and how do these criteria potentially influence the management strategies recommended in the guidelines?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: A healt hcare professional wit h higher training in r espirat ory medicine and pr oficiency in \n",
      "the management of ast hma. In t he cont ext of t his g ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What are the key recommendations for treating and monitoring hypertension according to the guidelines, and how do they address the need for specialist referrals and research contributions?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: 1.4 Treating and monit oring h yper tension .......................................................................................... 10 \n",
      "1.5 Identif ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What implications do the updated recommendations for managing hypertension in individuals with type 2 diabetes of Black African or African-Caribbean descent have for clinical practice, particularly regarding the transition from monotherapy to dual therapy?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: clinically equiv alent. \n",
      "For people of Black African or African–Caribbean f amily origin wit h type  2 diabet es, the \n",
      "previous r ecommendation was t  ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What alternatives does the committee suggest for diagnosing asthma in cases where standard tests like spirometry and FeNO are delayed or unavailable, particularly for children?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: The committ ee w ere awar e that t here can be dela ys in accessing spir ometr y and F eNO \n",
      "testing, and it is hoped t hat access will impr ove. Ho we ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What steps should be taken if a patient’s asthma remains uncontrolled despite treatment with a moderate-dose MART regimen, and what factors should be assessed to guide further management?\n",
      "Answer A: What should be done if treatment with MART using a moderate-dose maintenance regimen does not provide adequate asthma control?\n",
      "Answer B: The recommendation for MART as the preferred step-up treatment is new, but it is not intended for children who are stable on current therapy, and introducing it should not be disruptive. It will bring advantages in terms of reducing asthma attacks and will be cost-effective for the NHS.\n",
      "Source chunk preview: If treatment wit h MART using a lo w-dose maint enance r egimen does not pr ovide adequat e \n",
      "asthma contr ol, the committ ee agr eed t hat incr easing ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: How can commissioners and providers effectively incorporate environmental sustainability into the implementation of NICE recommendations for asthma diagnosis and management?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: Commissioners and pr oviders ha ve a responsibility t o promot e an en vironmentally \n",
      "sustainable healt h and car e syst em and should assess and r ed ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What are the key considerations outlined in the new asthma management guideline for children aged 5 to 11, particularly regarding medication combinations, self-management strategies, and risk-stratified care?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: Medicine combination and sequencing in childr en aged 5 t o 11 ...................................................... 56 \n",
      "Pharmacological management i ...\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What was the rationale behind the committee's decision to not retain the previous recommendation for beta-blockers as step 1 antihypertensive treatment, and how has the management of blood pressure for adults with type 2 diabetes been updated in the new guidelines?\n",
      "Answer A: I don't know.\n",
      "Answer B: I don't know.\n",
      "Source chunk preview: that t he pr evious r ecommendations f or st ep 1 treatment should be r etained (wit h minor \n",
      "changes f or clarity), because t hey were based on r obu ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, chunks, embedder, client):\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        answer = answer_query(question, k, index, chunks, embedder, client)\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results_a = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_a,\n",
    "    chunks=chunks_a,\n",
    "    embedder=embedder_a,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Embedder B: Wrapper for OpenAI embeddings\n",
    "# Provides an .encode() method compatible with retrieve_texts()\n",
    "# ============================================================\n",
    "\n",
    "class OpenAIEmbedder:\n",
    "    def __init__(self, client, model=\"text-embedding-3-small\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def encode(self, texts, convert_to_numpy=True):\n",
    "        # Call OpenAI embeddings\n",
    "        resp = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=texts\n",
    "        )\n",
    "        # Convert embeddings to numpy array\n",
    "        vectors = np.array([item.embedding for item in resp.data])\n",
    "        return vectors\n",
    "\n",
    "\n",
    "# instantiate embedder_b\n",
    "embedder_b = OpenAIEmbedder(openai_client)\n",
    "\n",
    "results_b = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_b,\n",
    "    chunks=chunks_b,\n",
    "    embedder=embedder_b,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "\n",
    "for item_a, item_b in zip(results_a, results_b):\n",
    "    print(\"Question:\", item_a[\"question\"])\n",
    "    print(\"Answer A:\", item_a[\"answer\"])\n",
    "    print(\"Answer B:\", item_b[\"answer\"])\n",
    "    print(\"Source chunk preview:\", item_a[\"chunk\"][:150], \"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebdb9b",
   "metadata": {},
   "source": [
    "# Build Markdown comparison table for RAG A vs B\n",
    "\n",
    "table_lines = []\n",
    "table_lines.append(\"| Question | Answer A | Answer B |\")\n",
    "table_lines.append(\"|----------|----------|----------|\")\n",
    "\n",
    "for item_a, item_b in zip(results_a, results_b):\n",
    "    q = item_a[\"question\"]\n",
    "\n",
    "    # shorten answers for table readability\n",
    "    ans_a = item_a[\"answer\"].replace(\"\\n\", \" \")[:120] + \"...\"\n",
    "    ans_b = item_b[\"answer\"].replace(\"\\n\", \" \")[:120] + \"...\"\n",
    "\n",
    "    # escape markdown pipes\n",
    "    q = q.replace(\"|\", \"\\\\|\")\n",
    "    ans_a = ans_a.replace(\"|\", \"\\\\|\")\n",
    "    ans_b = ans_b.replace(\"|\", \"\\\\|\")\n",
    "\n",
    "    row = f\"| {q} | {ans_a} | {ans_b} |\"\n",
    "    table_lines.append(row)\n",
    "\n",
    "markdown_table = \"\\n\".join(table_lines)\n",
    "print(markdown_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec212b7",
   "metadata": {},
   "source": [
    "Add RAG B and create a comparison table\n",
    "\n",
    "If you implemented a second configuration (e.g. different chunking + OpenAI embeddings):\n",
    "\n",
    "1. Build `index_b` and `embedder_b`.\n",
    "2. Run `results_b = answer_generated_questions(..., index_b, chunks_b, embedder_b, client)`.\n",
    "3. For each question, compare:\n",
    "   - Which answer is more complete / specific?\n",
    "   - Which one is better grounded in the source chunk?\n",
    "4. Summarise your findings in a short **markdown cell** or a small table.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the core RAG exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
